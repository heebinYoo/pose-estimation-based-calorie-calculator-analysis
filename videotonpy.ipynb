{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "from enum import Enum, auto, IntEnum\n",
    "\n",
    "import tensorflow as tf  # TF2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "class BodyPart(IntEnum):\n",
    "\n",
    "    NOSE = 0\n",
    "    LEFT_EYE = auto()\n",
    "    RIGHT_EYE = auto()\n",
    "    LEFT_EAR = auto()\n",
    "    RIGHT_EAR = auto()\n",
    "    LEFT_SHOULDER = auto()\n",
    "    RIGHT_SHOULDER = auto()\n",
    "    LEFT_ELBOW = auto()\n",
    "    RIGHT_ELBOW = auto()\n",
    "    LEFT_WRIST = auto()\n",
    "    RIGHT_WRIST = auto()\n",
    "    LEFT_HIP = auto()\n",
    "    RIGHT_HIP = auto()\n",
    "    LEFT_KNEE = auto()\n",
    "    RIGHT_KNEE = auto()\n",
    "    LEFT_ANKLE = auto()\n",
    "    RIGHT_ANKLE = auto()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "def getPoseScale(Coords):\n",
    "    sholder_middle = (Coords[BodyPart.RIGHT_SHOULDER] + Coords[BodyPart.LEFT_SHOULDER])/2\n",
    "    hip_middle = (Coords[BodyPart.RIGHT_HIP] + Coords[BodyPart.LEFT_HIP])/2\n",
    "    dist = [np.linalg.norm(sholder_middle-hip_middle)*2.5]\n",
    "    for point in range(17):\n",
    "        dist.append(np.linalg.norm(Coords[point]))\n",
    "    return np.max(dist)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "interpreter = tf.lite.Interpreter(\n",
    "    model_path=\"posenet_model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "floating_model = input_details[0]['dtype'] == np.float32\n",
    "\n",
    "\n",
    "img_height = input_details[0]['shape'][1]\n",
    "img_width = input_details[0]['shape'][2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/1568 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a6ce8c83a2c4fdfbb2e18a2131744ea"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# \"./data/barbel/barbel.mov\",\n",
    "#                \"./data/barbel/barbel-stable.mov\",\n",
    "#                \"./data/barbel/barbel-up.mov\",\n",
    "#                \"./data/legwork/legwork.MOV\",\n",
    "#                \"./data/legwork/legwork-up.MOV\",\n",
    "#                \"./data/legwork/stable-down.mp4\",\n",
    "#                \"./data/squat/Squat.mp4\",\n",
    "#                \"./data/squat/squat-up.mp4\",\n",
    "video_files = [\n",
    "\n",
    "               \"./data/squat/squat-down.mov\"\n",
    "               ]\n",
    "\n",
    "\n",
    "\n",
    "for video_file in video_files:\n",
    "\n",
    "    from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "    cap = cv2.VideoCapture(video_file)  # 동영상 캡쳐 객체 생성  ---①\n",
    "\n",
    "\n",
    "\n",
    "    result_data = list()\n",
    "    checked = False\n",
    "    if cap.isOpened():  # 캡쳐 객체 초기화 확인\n",
    "        for i in tqdm(range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT)))):\n",
    "            ret, img = cap.read()  # 다음 프레임 읽기      --- ②\n",
    "            if ret:  # 프레임 읽기 정상\n",
    "                    img = cv2.resize(img, dsize=(img_width, img_height))\n",
    "                    img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "                    if not checked:\n",
    "                        cv2.imshow(video_file, img)  # 화면에 표시  --- ③\n",
    "                        cv2.waitKey(1)  # 25ms 지연(40fps로 가정)   --- ④\n",
    "                        checked = True\n",
    "                    input_data = np.expand_dims(img, axis=0)\n",
    "                    if floating_model:\n",
    "                        input_data = (np.float32(input_data) - 127.5) / 127.5\n",
    "                    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "                    interpreter.invoke()\n",
    "                    heatmaps = interpreter.get_tensor(output_details[0]['index'])\n",
    "                    offsets = interpreter.get_tensor(output_details[1]['index'])\n",
    "\n",
    "                    height = heatmaps.shape[1]\n",
    "                    width = heatmaps.shape[2]\n",
    "                    numKeypoints = heatmaps.shape[3]\n",
    "\n",
    "                    keypointPositions = [0 for i in range(numKeypoints)]\n",
    "\n",
    "                    for keypoint in range(numKeypoints):\n",
    "                        maxVal = heatmaps[0][0][0][keypoint]\n",
    "                        maxRow = 0\n",
    "                        maxCol = 0\n",
    "                        for row in range(height):\n",
    "                            for col in range(width):\n",
    "                                if heatmaps[0][row][col][keypoint] > maxVal:\n",
    "                                    maxVal = heatmaps[0][row][col][keypoint]\n",
    "                                    maxRow = row\n",
    "                                    maxCol = col\n",
    "\n",
    "                        keypointPositions[keypoint] = (maxRow, maxCol)\n",
    "\n",
    "                    # xCoords = [0 for i in range(numKeypoints)]\n",
    "                    # yCoords = [0 for i in range(numKeypoints)]\n",
    "                    Coords = [0 for i in range(numKeypoints)]\n",
    "                    confidenceScores = [0 for i in range(numKeypoints)]\n",
    "\n",
    "                    for idx, position in enumerate(keypointPositions):\n",
    "                        positionY = keypointPositions[idx][0]\n",
    "                        positionX = keypointPositions[idx][1]\n",
    "                        yCoords = position[0] / (height - 1) * img_height + offsets[0][positionY][positionX][idx]\n",
    "                        xCoords = position[1] / (width - 1) * img_width + \\\n",
    "                                       offsets[0][positionY][positionX][idx + numKeypoints]\n",
    "                        Coords[idx] = [xCoords, yCoords]\n",
    "                        confidenceScores[idx] = sigmoid(heatmaps[0][positionY][positionX][idx])\n",
    "                    Coords = np.array(Coords)\n",
    "\n",
    "                    # recenterize\n",
    "                    center_of_mass = (Coords[BodyPart.LEFT_HIP] + Coords[BodyPart.RIGHT_HIP])/2\n",
    "                    Coords = Coords - center_of_mass\n",
    "\n",
    "                    # normalize\n",
    "                    norm_factor = getPoseScale(Coords)\n",
    "                    Coords = Coords/norm_factor\n",
    "                    result_data.append(Coords)\n",
    "            else:\n",
    "                break\n",
    "    else:\n",
    "        print(\"can't open video.\")  # 캡쳐 객체 초기화 실패\n",
    "    cap.release()  # 캡쳐 자원 반납\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    result_data = np.array(result_data)\n",
    "    np.save(video_file+\".npy\",result_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}